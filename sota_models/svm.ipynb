{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "birth_below_scan_above       0.78      0.92      0.84        50\n",
      "      birth_scan_above       0.92      0.72      0.81        47\n",
      "      birth_scan_below       0.96      0.98      0.97        56\n",
      "\n",
      "              accuracy                           0.88       153\n",
      "             macro avg       0.89      0.88      0.88       153\n",
      "          weighted avg       0.89      0.88      0.88       153\n",
      "\n",
      "tensor([[0.7797, 0.9189, 0.9649],\n",
      "        [0.9200, 0.7234, 0.9821],\n",
      "        [0.8738, 0.9717, 0.9794]])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "birth_below_scan_above       0.50      0.83      0.62        36\n",
      "      birth_scan_above       0.82      0.53      0.65        60\n",
      "      birth_scan_below       0.96      0.91      0.94        57\n",
      "\n",
      "              accuracy                           0.75       153\n",
      "             macro avg       0.76      0.76      0.74       153\n",
      "          weighted avg       0.80      0.75      0.75       153\n",
      "\n",
      "tensor([[0.5000, 0.8205, 0.9630],\n",
      "        [0.8333, 0.5333, 0.9123],\n",
      "        [0.7436, 0.9247, 0.9792]])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "birth_below_scan_above       0.64      0.71      0.67        48\n",
      "      birth_scan_above       0.73      0.64      0.68        58\n",
      "      birth_scan_below       0.90      0.94      0.92        47\n",
      "\n",
      "              accuracy                           0.75       153\n",
      "             macro avg       0.75      0.76      0.76       153\n",
      "          weighted avg       0.75      0.75      0.75       153\n",
      "\n",
      "tensor([[0.6415, 0.7255, 0.8980],\n",
      "        [0.7083, 0.6379, 0.9362],\n",
      "        [0.8190, 0.8526, 0.9528]])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "birth_below_scan_above       0.68      0.86      0.76        49\n",
      "      birth_scan_above       0.84      0.64      0.73        56\n",
      "      birth_scan_below       0.96      0.96      0.96        48\n",
      "\n",
      "              accuracy                           0.81       153\n",
      "             macro avg       0.82      0.82      0.81       153\n",
      "          weighted avg       0.82      0.81      0.81       153\n",
      "\n",
      "tensor([[0.6774, 0.8372, 0.9583],\n",
      "        [0.8571, 0.6429, 0.9583],\n",
      "        [0.8077, 0.9278, 0.9810]])\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "birth_below_scan_above       0.63      0.76      0.69        55\n",
      "      birth_scan_above       0.70      0.54      0.61        48\n",
      "      birth_scan_below       0.96      0.94      0.95        50\n",
      "\n",
      "              accuracy                           0.75       153\n",
      "             macro avg       0.76      0.75      0.75       153\n",
      "          weighted avg       0.76      0.75      0.75       153\n",
      "\n",
      "tensor([[0.6269, 0.7027, 0.9592],\n",
      "        [0.7636, 0.5417, 0.9400],\n",
      "        [0.7449, 0.8952, 0.9806]])\n",
      "tensor([[0.6451, 0.8010, 0.9487],\n",
      "        [0.8165, 0.6158, 0.9458],\n",
      "        [0.7978, 0.9144, 0.9746]])\n",
      "tensor([0.7982, 0.7927, 0.8956])\n"
     ]
    }
   ],
   "source": [
    "X = []  # 定义图像名称\n",
    "Y = []  # 定义图像分类类标\n",
    "\n",
    "with open('fc_list.txt','r') as f:\n",
    "    data=f.readlines()\n",
    "data=[item.replace('\\n','').split('\\t') for item in data]\n",
    "random.seed(2022)\n",
    "random.shuffle(data)\n",
    "result_index_matrix=torch.zeros(5,3,3)\n",
    "\n",
    "k_fold=5\n",
    "for kk in range(k_fold):\n",
    "    k_fold_len = int(len(data)//k_fold)\n",
    "    test_data = data[k_fold_len*kk:k_fold_len*(kk+1)]\n",
    "    train_data = data[:k_fold_len*kk] + data[k_fold_len*(kk+1):]\n",
    "\n",
    "    X_train=[x[0] for x in train_data]\n",
    "    y_train=[x[1] for x in train_data]\n",
    "    X_test=[x[0] for x in test_data]\n",
    "    y_test=[x[1] for x in test_data]\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_test=np.array(X_test)\n",
    "    y_test=np.array(y_test)\n",
    "\n",
    "    XX_train = []\n",
    "    for imagepath in X_train:\n",
    "        # temp_image=h5py.File(imagepath,'r')\n",
    "        temp_image=scio.loadmat(imagepath)['fc']\n",
    "        XX_train.append(temp_image.flatten())  #reshape into 1D array\n",
    "\n",
    "    # 测试集\n",
    "    XX_test = []\n",
    "    for imagepath in X_test:\n",
    "        # temp_image=h5py.File(imagepath,'r')\n",
    "        temp_image=scio.loadmat(imagepath)['fc']\n",
    "        XX_test.append(temp_image.flatten())\n",
    "\n",
    "    # 第三步 基于支持向量机的图像分类处理\n",
    "    # 0.5\n",
    "    # 常见核函数‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "    clf = SVC().fit(XX_train, y_train)\n",
    "    clf = SVC(kernel=\"linear\").fit(XX_train, y_train)\n",
    "    predictions_labels = clf.predict(XX_test)\n",
    "\n",
    "    c_matrix=confusion_matrix(y_test,predictions_labels)\n",
    "    for idm in range(3):\n",
    "        result_index_matrix[kk][0][idm]=c_matrix[idm][idm]/np.sum(c_matrix[:,idm])\n",
    "        result_index_matrix[kk][1][idm]=c_matrix[idm][idm]/np.sum(c_matrix[idm,:])\n",
    "        result_index_matrix[kk][2][idm]=(np.sum(c_matrix)-np.sum(c_matrix[:,idm])-np.sum(c_matrix[idm,:])+c_matrix[idm][idm])/(np.sum(c_matrix)-np.sum(c_matrix[idm,:])) \n",
    "\n",
    "    print(classification_report(y_test, predictions_labels))\n",
    "    print(result_index_matrix[kk])\n",
    "mean_matrix=result_index_matrix.sum(dim=0)/5\n",
    "print(mean_matrix)\n",
    "print(mean_matrix.sum(dim=1)/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
